\documentclass[12pt]{extreport}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\def\BState{\State\hskip-\ALG@thistlm}

\begin{document}

\begin{titlepage}
 \begin{center}

  \vspace*{1cm}

  \begin{huge}
   \textbf{Autonomous Robots using Reinforcement Learning}
  \end{huge}

  % \begin{large}
  \vspace{0.5cm}

  \begin{multicols}{2}
   \textbf{Nikhil Verma}\\
   2014A3PS200P

   \textbf{Amey Agrawal}\\
   2014A7PS148P
  \end{multicols}

  \vspace{1cm}

  \includegraphics[width=0.4\textwidth]{logo}
  \vspace{1cm}

  Birla Institute of Technology and Science, Pilani\\
  \vspace{0.5cm}
  Final Report submitted in the fulfillment of the course\\
  \textbf{EEE/CS F491 Special Project}\\
  Under the Supervision of\\
  \textbf{Prof. Surekha Bhanot}\\
  \vspace{0.5cm}
  \today

  % \end{large}

 \end{center}
\end{titlepage}

\tableofcontents

\chapter{Introduction}


Humans have forever wanted to make autonomous systems capable of making intelligent decisions on their own. The recent advancements in AI year after year are moving us closer to realizing this dream. This project is an attempt to convert these pioneering ideas into real world applications, making the world a better place.

In 2013, a small company in London by the name DeepMind published its paper \emph{Playing Atari with Deep Reinforcement Learning} \cite{mnih2013atari}.
In this paper they demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable, because the games and the goals in every game were very different and designed to be challenging for humans. The same model architecture, without any change, was used to learn seven different games, and in three of them the algorithm performed even better than a human!

In February 2015 their paper \emph{Human-level control through deep reinforcement learning} \cite{mnih2015nature} was featured on the cover of Nature, one of the most prestigious journals in science. In this paper they applied the same model to 49 different games and achieved superhuman performance in half of them.

Remember that the same architecture was used to learn a variety of different games. Now this was a major breakthrough and is often cited as the first step towards general artificial intelligence â€“ an AI that can survive in a variety of environments, instead of being confined to strict realms such as playing chess.

These pioneering innovations inspired us to build autonomous systems capable of making all decisions by itself, without any external aid. An simple and interesting application would be to teach mobile robots to move from one place to another, while making all intermediate decisions on its own. This would be very effective in warehouse like environments where multiple robots operate within the same region. It would be interesting to see how the bots react when about to collide.

Through this project we demonstrate the possibility of using deep reinforcement learning to build autonomous robots in real life, and the whole world of possibilities that it opens for us.


\chapter{Algorithm}

\section{Deep Q-learning}

In the reinforcement learning literature, Q-learning can be defined as an algorithm which tries to address the credit assignment problem. Basically we have an agent which has to learn to perform a specific task (say, learn to follow a path). The way the algorithm works is that it tries to give a suitable reward (as positive scores) if the bot follows the path, but also gives large penalties (negative scores) if it commits a mistake. Mathematically, the ultimate objective of the agent is to work out in such a way to maximize the reward.
The robotâ€™s path can be modelled as a Markov Decision Process, where each action (turn, go straight, stop, etc) can be predicted by the algorithm based on the current (say, the current position of the bot) inputs (line sensors). The problem can be modelled as a collection of states and actions, and the bot tries to predict the next action based on the current state. This is done by using the Q learning algorithm. When the same prediction is done using deep neural networks, this is called Deep Q-Learning.
Here is a pseudocode for the algorithm,

\begin{algorithm}
 \caption{Deep Q-learning}\label{euclid}
 \begin{algorithmic}[1]
  \State Initialize action-value function Q with random weights
  \State Observe initial state s
  \For {episode = 1, M}
  \For {steps = 1, T}
  \State With probability select a random action $ a_t $
  \State Otherwise select $ a_t = max_a Q âˆ— (\phi(s_t), a, \theta) $
  \State Execute action at in emulator and observe reward $ r_t $ and image $ x_{t+1} $
  \State $ Set s_{t+1} = s_t, a_t, x_{t+1} $ and preprocess $ \phi_{t+1} = \phi(st+1) $
  \State $ y_j = r_j + \gamma max_{a'} Q(\phi_{j+1}, a'; \theta) $
  \State Perform a gradient descent step on $ (y_j - Q(\phi_j , a_j ; \theta))^2 $
  \EndFor
  \EndFor
 \end{algorithmic}
\end{algorithm}

\section{Neural Network Architecture}

We use a four layer deep multi-layer perceptron model. With ReLU activation for
the input and hidden layers while softmax is applied on the output later.

The number of units in layers are set according to the task to be performed.
Adam is used for backpropogation with cross-entropy loss.

\chapter{Experimental Setup}

\section{Tasks}

\subsection{Path follow}
Path following has been sucessfully trid on many video games using deep
reinforcement learning. But all of them take the
We trained an agent with with IR sensor array to

\section{Simulation} \label{ch:simulation}

\subsection{The Need for Simulation}

When a agent learns to perform actions using reinforcement learning, it
needs to interact with the environment. Though in principle it could
done using a physical robot, training a neural network on a physical bot posed
a lot of challenges.

\begin{itemize}
 \item The bot had to be placed on its starting position after every training episode. With more than 500 episodes to train, this would be cumbersome and impractical.
 \item Because of the minimum speed discussed in \autoref{sec:testpretrainedmodels}, the bot would often run out of track, collide with barriers, or move in a random fashion. This should be avoided during training. In any such case, say, running out of the track, the bot can only return to the track by chance. The sensors would be ineffective, so there's no point in continuing that episode.
 \item Once the bot moved out of the chart paper on which the actual track was inscribed, it would receive abrupt inputs because of the rough surface beyond the chart paper. This would further hamper the learning process.
 \item In case of any of the above incidents, stopping the iteration manually involved a lot of latency. This uncertainity is not desired in the training process.
\end{itemize}

Hence, real-like simulations are essential to train and expirement with
algorithums. The simulation techniques we employed are discribed bellow.

\subsection{Simulation Techniques}

We used OpenAI gym as our primary interface the learning algorithm with the
simulation envinorment. Initially we developed 2D simulations in Box2D and
later a more rhobust system was developed using V-Rep.

\subsubsection{BOX2D}
Box2D is 2D simulation library in C++ with python bindings named as pybox2D.
As pybox2D is part of the original OpenAI gym simulation environments.
We developed a simulation for path follwing using


\subsubsection{V-REP}


\section{Physical Simulation}


The aim of this project was also to demonstrate the use of the algorithm in the real world. To serve this purpose, we built a physical model of a mobile robot using readily available components.

\subsection{Components}
The following major components were used in the physical model of the bot.
\begin{itemize}
 \item Acrylic Chassis with 2 Wheel Drive
 \item 8 Channel IR Sensor Array
 \item LM293D Motor Driver board with 7805 Power Supply
 \item 4 Ultrasonic Range Finder Modules
 \item 9V Power Adapter
 \item Raspberry Pi (Model B)
\end{itemize}

\subsection{The Microcontroller}
Both, the Raspberry Pi and the Arduino could be used as the microcontroller for the bot. But since the Raspberry Pi runs Python out of the box, it was the obvious choice when it comes to machine learning. Also, libraries such as TensorFlow are specially designed to run on Raspberry Pi.

It turns out that we eventually performed all the learning and decision making on a computer, with the Pi only serving as an interface to control the bot. In such a scenario, the Arduino would work equally well, except the fact that it would need another means of communication than described below.

\subsection{Networking and Communications}
The Raspberry Pi on the bot was connected to a home WiFi network using an external WiFi adapter. Our computer was also connected to the same network.
An SSH server was enabled on the Pi which allowed accessing it remotely from the computer. All of the programs were uploaded using SSH.

Testing pretrained models trained in \autoref{ch:simulation} on the bot had its own issues. The python script on the computer needed to communicate with the bot in order to get sensor values. We needed some form of interprocess communication, which was not possible natively using SSH.

\subsubsection{ZeroMQ}
We tried out ZeroMQ, the distributed messaging library. Although we were able to exchange close to 400 messages per second, we later realized that we also needed to provide the bot with actions computed by the network. This posed a problem because ZeroMQ uses a blocking queue. The communications would freeze very often on the occasion of frequent deadlocks.

\subsubsection{Flask}
HTTP was the next best available mode of communication and \texttt{Flask} was the next best tool for the purpose. Being lightweight, it did not hamper connection speed. Also, it runs quite well on the RPi. Since HTTP was built to support multiple channels and because it was non blocking, this solution worked quite well.

So we set up a \texttt{Flask} server on the RPi and used the \texttt{requests} package to send \texttt{POST} requests on the Flask server with actions, which, in turn, would return the observations of the sensors.

\subsection{Testing Pretrained Models} \label{sec:testpretrainedmodels}
For the few experiments we performed while testing the pretrained models trained in \autoref{ch:simulation}, the results suggested a few changes that needed to be made before achieving decent results. The following challenges were faced.

\begin{itemize}
 \item There was a considerable difference in the size of the actual track and the track used while simulation. We did not take into account this fact while making the simulation.
 \item The rate of acquiring observations in the real world was significantly faster than that from simulations. We believe this is because a sizeable chunk of the computer's resources were being used in simulation, resulting in slower data acquisition.
 \item The actual bot was considerably heavy and the motors didn't provide enough torque to move the bot at lower speeds. This resulted in a \emph{minimum speed} of the bot, below which it refulsed to move.
\end{itemize}

\subsection{Training Physical Model}

\bibliographystyle{apalike}
\bibliography{references}

\end{document}
